AI, you thirsty customer

Hi. I'm Oli, and a while ago I stumbled across this headline:

"ChatGPT drinks 500ml of water for every conversation. And that's scary."
https://medium.com/@aiuniverse/chatgpt-drinks-500ml-of-water-for-every-conversation-and-that-s-scary-e8558a77cd36

With my interest piqued, I started googling and found articles about water consumption in Iowa
(where GPT3 and four are trained) and other articles about how data centres from the big tech
companies are taking high amounts of water out of their local water systems.

https://apnews.com/article/chatgpt-gpt4-iowa-ai-water-consumption-microsoft-f551fde98083d17a7e8d904f8be822c4
https://www.theregister.com/2023/04/11/microsoft_evaporative_coolers_arizona/

The tune sang within these is mostly the same: 
- Data centre water consumption has risen in the recent years
- Main culprits are direct water consumption by cooling and indirect water consumption used by power generation
- An especially high increase in water consumption coincides with the AI boom
- In some regions, the water consumed by data centre strains the local water supply

This is all well and good, but what about that "500ml per conversation" claim?

Most sources seem to cite this paper: 
https://arxiv.org/abs/2304.03271

Important note here: This is a relativley short paper on Arxive.org
Arxive.org is a preprint archive. Papers on here have been moderated but not 
peer reviewed. The paper does cite its sources however. I invite you to do your own check on the paper 
and it sources to find discrepancies. 

Now, what is in the paper?

Since we are not sitting together to scroll through text, let me jump to the main bits of the paper.

This is probably the most interesting table in the whole paper, where the researchers estimate 
the water consumption of a gpt-3 model with 185 billion parameters.

Lets have a look at the row for Iowa.
The data estimates that a GPT3 deployed in Microsofts Iowa data centre  would consume about 5 million 
litres of water for training and about 15ml of water per "inference" given the three 
numbers for efficiency at the start of the table. These number estimate the amount of water that is used
per kilowatts per hour in a given location, either by the data centre itself or the power plants in 
the electrical gid it is connected to. What they basically do is attach a kWh cost to each inference and 
then calculate the water consumption for that.
The handy column at the end of the table shows the estimate of how many inferences can be had for 
500ml water consumption. 

Ok, so they talk about "inferences". What is that. The way it is explained in the paper the scientists first took the 
data for how much water is consumed per kWh for the data centre itself (WUE) and for the power grid 
the data center is embedded into (Electricity Water Intensity). Then they estimated a kWh number of 
electricity use per interaction. They first talk about an official estimate that names 0.4kWh per 100
pages, or 0.004kWh per page. Then they name another study, where an LLM called BLOOM was measured 
and found 0.004kWh per request and conclude that this is the number they will use. 
But the table talks about "inferences", which is implicitly equated to "request". 
I admit that I do not fully understand the exact connection between inferences, pages and requests-
but I am not a scientist, so the error is probably on my part. For the context of this talk,
I will take it at face value that asking ChatGPT one question constitutes one request or inference.

I have here this conversation with ChatGPT 3.5. Provided that it is hosted in Iowa and that the data 
in the paper for GPT3 holds, the roundabout 18 requests would use up an estimated 
(15 ml by 18 exchanges) 270ml. When extended to the whole table, the estimated minimum lies between 
126ml (Dublin) and 860ml (Washington).

So the assumption that any one conversation with ChatGPT on average involves 500ml of water does in
fact seem credible.

There is another thing. The GPT3 Model behind ChatGPT has been retrained multiple times. If in January 2023
it had 100 million users a month and the first training cost was 5 million litres of water, this comes down 
to 0.05 litres per user, over each usage period between retrainings. This is of course a back of the envelope
calculation and NOT part of the paper, but make of that number what you will.
https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/

The question to ask now is of course, what do they mean by "drink" and how do they get to their numbers.
The "drink" part is easily explained. In their research, the scientists in the paper mention 
"water withdrawal" and "water consumption". Withdrawal is the amount of water taken out of a system, 
like a municipal water source. Consumption is the amount of water of the withdrawn water, that is not
put back into the system because of multiple factors, but in our case the main two reasons are 
"Pollution" and "Evaporation". Whenever the paper mentions the usage of water it uses the values 
for water consumption.

Where in the blazes does a server farm pollute and evaporate such an amount of water? In the paper, this 
question is broken down to Scope 1 and Scope 2 water consumption. 

As in GHG reporting, Scope 1 is the water that is running down the pipes directly connected to the data 
centre and used on-site. Scope 2 is the water that is used to provide utilities to the data centre. In the 
paper the main consumer here is power production. Scope 3 would be the embodied water consumption from
hardware manufacturing. This part of the consumption is purposefully excluded, since no reliable data could 
be obtained by the scientists in the paper.

The main source of water consumption in data centres according to the paper is cooling. There are two main
types of cooling: direct air and evaporative cooling. When cooling with direct outside air a small but 
still significant amount of water is consumed via either cooling the outside air down above a certain
temperature (given as around 85° F/ 30° C) before blowing it through the system or for humidity control. 
The paper estimates an average water consumption of 0.2 litres per kWh for this type of cooling.
Evaporative cooling uses cooling towers that directly transport heat away from the cooling system by releasing
evaporated water into the atmosphere. Only some of the water in the open cooling cycle of such a system 
evaporates, but since evaporation also concentrates pollutants in the water, all the water in the system needs 
to be exchanged after a few cooling cycles, making it waste water. The estimate for this type of cooling 
ranges form 1 litre water consumption per kWh to 9 litres per kWh in the worst case.
Since both types of cooling need clean water to minimize maintenance and, in case of evaporative cooling,
maximize cooling cycles, potable (read: drinking) water is used. This is corroborated by multiple municipalities 
around the globe either stopping the construction of data centres or feeling the strain on available water 
in their systems from data centre operation. 
https://www.datacenterdynamics.com/en/news/drought-stricken-holland-discovers-microsoft-data-center-slurped-84m-liters-of-drinking-water-last-year/
https://apnews.com/article/chatgpt-gpt4-iowa-ai-water-consumption-microsoft-f551fde98083d17a7e8d904f8be822c4

Scope 2 water usage of a data centre is attributed to water consumption wile power is generated. Even with the 
advent of renewable power sources, electricity is mainly generated in thermoelectric plants around the globe.
In the column "Electricity Water Intensity" the scientists are naming the numbers they found for how much 
water is consumed per kWh produced in a local grid. 

AI, you messy customer

Drawing on the conclusions of the paper, lets do some carbon emissions calculations. The screenshot shown is 
electricitymaps.com, which shows the Carbon Emissions per kilowatt hours produced by country. In this case a cloudy 
June morning in Germany. The given carbon intensity of 400g/CO2 is close to yearly estimates from the same data, so let's use that.
For my 18 requests at 0.004 kWh this would come to 0.03grams of CO2 equivalents.

But there are more sources we could look at. This is green-coding.ai, an experimental project by green-coding.io, a 
software consultancy firm from Berlin. They have a cluster on-site that is running a number of LLMs. So they can simply measure
the power needs of different LLMs. For one request they come out with 0.4 grams for an example request on a slightly rainy
morning, also in June. This is quite a bit more pessimistic, as a 18 request conversation would result in 7.2grams of CO2

Any more data? DATEV has its own GPT-4 instances running on Azure. Shoutout and thanks to Markus Glas and the GenAIlize Colleagues
to let me have access to their data.
DATEV-GPT is used frequently, with 23.000 calls in April and May and a Carbon Footprint of around 400 kg in that same time frame.
Which would come out to 2 gram of CO2 equivalents per call. 

Where does that rather large range of values come from. Well for one by their own standards, what the carbon dashboard of 
azure is reporting are scope 3 emissions. So basically emissions from manufacturing, not power usage while running.
Microsoft seems to think, that its power consumption does not create carbon dioxide. How come?

We can find that information here: The Microsoft global commitments. 

But in reality, electrtricity takes the way of least resistance. Imagine pouring the most pure mineral water imaginable into 
the danube at Donaueschingen and someone taking water out in Wylokwe, Ukraine. They will mainly be getting local water and 
pouring more and more mineral water into the danube will not result in them getting more mineral water- only in the most expensive
flodding of southern Germany imaginable. 

So how do things look in Poland? Lets first look at the global map of microsoft data centers. We will see that while microsoft has a 
number of renewable energy projects worldwide, there are none in Poland (or Germany for that matter).
A glance at electricitymaps for the last year in Poland shows an estimation of around 750grams CO2 per kWh produced. Which 
would add 3 grams of CO2 per call. So 5 grams in total when adding the 2 grams of scope 3 emission Microsoft is 
reporting. Which if we keep using the numbers from my slides, 
would be about 90 grams for my 18 requests. Assuming the data in this screenshot is correct, that would equal 450 google searches.
Or, according to WebsiteCarbon around 220 views of StackOverflow- or any mixture of the two.

A final word on image generation. In a paper- also on the Arxive- HuggingFace Sustainability Office Sasha Luccioni et. al. have 
looked at the relative carbon dioxide emissions of different machine learning tasks, from relatively trivial classification to 
GenAI use cases like text end image generation.
https://arxiv.org/abs/2311.16863
The shown image shows a climb in "cost"-meaning carbon emissions- by up to four orders of magnitude between single-purpose models
and multi-purpose models, with up to 1kg of emitted CO2 for 1000 image generations.

So, what to do?

First things first: I am standing here, talking to you as part of an industry- not individuals. I am not talking about your
"Personal Carbon Footprint" to absolve our industry from its responsibilities. I would like you as part of this industry 
to start the conversation on our collective responsibility.

What can we do? I primarily see two avenues.
The first is simple. Don't do GenAI. As it stands, GenAI is a waste and to the best of my knowledge nobody 
has made significant profits from running AI features yet. I would be very thankfull for your input if you have other
information.

The second, more reasonable approach is to pick our models and runtime environments carefully.

The big hyperscalers start to get more and more pushback to the nth datacenter they want to plop into a 
region stricken by drought and gobbling up all the water in the area.
At the same time they plan to build more of them, while sacrificing their sustainability goals of recent years and green-washing
the numbers they provide.

Microsoft, home of GPT 3 and 4 has doubled its water use since 2019, the year in which they promised to cut their water usage 
by 95%.

To me personally, the current hyperscalers are a no-go. But we can run our own models. There is a flood of Open Source 
LLMs of many sizes and specific use cases. Tools like Ollama and Spring AI make it easy to embed these into your own runtime
environment. And bigger companies have found the wisdom in using smaller, local processing where applicable.

Also overall, we seem to be steering into the "Valley of Tears", with the number of publications painting a more sombre
picture of GenAI that did one year ago.
 

Why do I care?

So lets take the estimates shown as face value for the moment. Why am I interested in that?
Here is where I leave the context of the paper and start supplying my own feelings and thoughts on the 
matter.

This is an overview of what Windows 11 has built in:
https://www.microsoft.com/en-us/windows/copilot-ai-features?r=1
On the one hand, this is basically Bing Chat crammed into the User Interface, but on the other hand there are 
"hidden" features that utilize AI to do their job.

The same for Adobe:
https://www.adobe.com/products/photoshop/ai.html
We have a DALL-e style image generator, and some "classical" Photoshop features that are now enhanced by 
Generative AI.

And one more, Github:
https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/
Apart from Copilot and Copilot Chat, Github plans to enhance every step of the developer journey, from 
Documentation to Pull requests to repository handling.

And here is where I start feeling a sting in the pit of my stomach. Lets assume that in the next few years 
many little tools that I am accustomed to start rolling out AI features that need to be processed in the cloud.
Lets furthermore assume that the 500ml water consumption number hold.
What that would mean is that many little tools switch from "makes my processor work a bit harder for a while" 
to "gulp down some small but significant amount of water and produce some amount of CO2". 
I find that deeply disturbing. That we will waste even more with every click of a button without even being 
aware of that.

I for one have stopped using AI tools for proloned time spans. I only use Github Copilot once in a while
to have an eye on the latest developments. I have no long conversations with ChatGPT and I
do not endlessly produce and fine-tune images with DALL-e and Midjourney.

What can you do? I do not know, nor would I dare to impose my worldviews on you. But there is one thing 
i would ask of you: Tomorrow in the shower, or at lunch or when ever your brain is reminding you of the things
you just heard: Make up your own mind. Form an opinion- and be honest to yourself. And if you would like to tell me 
that opinion you have I would be very interested in you sharing it with me.  








